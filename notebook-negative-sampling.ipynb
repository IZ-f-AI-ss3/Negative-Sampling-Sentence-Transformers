{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random as rd\n",
    "import random\n",
    "from typing import Dict, List, Literal\n",
    "from datasets import Dataset\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    "    SentenceTransformerTrainer,\n",
    "    SentenceTransformerTrainingArguments,\n",
    "    SentenceTransformerModelCardData,\n",
    ")\n",
    "from sentence_transformers.losses import MultipleNegativesRankingLoss, TripletLoss\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "\n",
    "from sentence_transformers import SentenceTransformerTrainer, SentenceTransformerTrainingArguments, SentenceTransformer\n",
    "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "\n",
    "\n",
    "train_data = pd.DataFrame('your train dataset ')\n",
    "eval_data = pd.DataFrame('your  eval dataset ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "link = \"all-MiniLM-L6-v2\"\n",
    "device = torch.device('mps')\n",
    "model = SentenceTransformer(link).to(device = device)\n",
    "train_dataset = Dataset.from_pandas(train_data) # train_data is a private pandas dataFrame\n",
    "eval_dataset = Dataset.from_pandas(eval_data)   # eval_data is a private pandas dataFrame\n",
    "\n",
    "# The DataFrame should follow this column naming convention: \"anchor\", \"positive\", \"negative_1\", ..., \"negative_{num_negatives}\".\n",
    "# Only \"negative_1\" will contain our full set of negative samples, while the other negative columns will be set to None.\n",
    "# These remaining negative columns will be populated dynamically for each batch.\n",
    "\n",
    "num_negatives = 5\n",
    "def sample_positive_negatives(batch):\n",
    "    anchors, positives, negatives = batch[\"anchor\"], batch[\"positive\"] , batch[\"negative_1\"]\n",
    "\n",
    "    sampled_batch = {\n",
    "    'anchor': [],\n",
    "    'positive': []\n",
    "    }\n",
    "    for i in range(1, num_negatives + 1):\n",
    "        sampled_batch[f'negative_{i}'] = []\n",
    "\n",
    "    for anchor, pos_list, neg_list in zip(anchors, positives, negatives):\n",
    "        sampled_positive = random.choice(pos_list)  \n",
    "        sampled_negatives = random.choices(neg_list, k=20) \n",
    "        sampled_batch[\"positive\"].append(sampled_positive)\n",
    "        sampled_batch[\"anchor\"].append(anchor)\n",
    "\n",
    "        for id in range(num_negatives) :\n",
    "            sampled_batch[f\"negative_{id+1}\"].append(sampled_negatives[id])     \n",
    "\n",
    "    return sampled_batch\n",
    "\n",
    "\n",
    "# This allows the random sampling with the Trainer method :\n",
    "train_dataset.set_transform(sample_positive_negatives)\n",
    "\n",
    "loss = MultipleNegativesRankingLoss(model).to(device)\n",
    "\n",
    "#  Specify training arguments\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=\"models/mpnet-base-all-nli-triplet\",\n",
    "    num_train_epochs= 5,\n",
    "    per_device_train_batch_size = 16, # 2\n",
    "    per_device_eval_batch_size= 4, # 2\n",
    "    warmup_ratio=0.1, \n",
    "    fp16=False,  # Set to False if you get an error that your GPU can't run on FP16\n",
    "    bf16=True ,   # Set to True if you have a GPU that supports BF16\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=3000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1,\n",
    "    save_total_limit=1,\n",
    "    logging_steps=100,\n",
    "    run_name=\"mpnet-base-all-nli-triplet\" )\n",
    "\n",
    "#  Create a trainer and train\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset , \n",
    "    loss=loss,\n",
    "    eval_dataset = eval_dataset\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
